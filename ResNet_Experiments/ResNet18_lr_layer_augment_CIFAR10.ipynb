{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "OTQZZ_syBi7T",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.318564Z",
     "start_time": "2024-04-26T13:15:40.285522Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cMEqfs0XO4f",
    "outputId": "1961f180-a0ca-441a-8c0b-1b388fc011e7",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.363731Z",
     "start_time": "2024-04-26T13:15:40.292951Z"
    }
   },
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.2.2\n",
      "Torchvision Version:  0.17.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set General Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 128\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 8\n",
    "\n",
    "# Percentage of the total dataset\n",
    "subset_percentage = 0.001\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ],
   "metadata": {
    "id": "ub3EVq0AXTjb",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.370936Z",
     "start_time": "2024-04-26T13:15:40.296462Z"
    }
   },
   "execution_count": 116,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True"
   ],
   "metadata": {
    "id": "pUFHOuNSXe-d",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.371052Z",
     "start_time": "2024-04-26T13:15:40.299317Z"
    }
   },
   "execution_count": 117,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "metadata": {
    "id": "QW4sVnZAXVmh",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.372507Z",
     "start_time": "2024-04-26T13:15:40.306988Z"
    }
   },
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "# These values are mostly used by researchers as found to very useful in fast convergence\n",
    "img_size=224\n",
    "crop_size = 224"
   ],
   "metadata": {
    "id": "aEkfO8jAXkrt",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.372617Z",
     "start_time": "2024-04-26T13:15:40.309559Z"
    }
   },
   "execution_count": 119,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize(img_size),#, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "     #transforms.CenterCrop(crop_size),\n",
    "     transforms.RandomRotation(20),\n",
    "     transforms.RandomHorizontalFlip(0.1),\n",
    "     transforms.ColorJitter(brightness=0.1,contrast = 0.1 ,saturation =0.1 ),\n",
    "     transforms.RandomAdjustSharpness(sharpness_factor = 2, p = 0.1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean,std),\n",
    "     transforms.RandomErasing(p=0.75,scale=(0.02, 0.1),value=1.0, inplace=False)])\n",
    "\n",
    "transformTest = transforms.Compose(\n",
    "[\n",
    "    transforms.Resize((img_size,img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])"
   ],
   "metadata": {
    "id": "i-aUbYk2Xmmx",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:40.372672Z",
     "start_time": "2024-04-26T13:15:40.315652Z"
    }
   },
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "subset_size = int(subset_percentage * len(full_dataset))\n",
    "subset_indices = torch.randperm(len(full_dataset))[:subset_size]\n",
    "subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "trainloader = torch.utils.data.DataLoader(subset_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transformTest)\n",
    "subset_size_test = int(subset_percentage * len(testset))\n",
    "subset_indices = torch.randperm(len(testset))[:subset_size_test]\n",
    "subset_testset = torch.utils.data.Subset(testset, subset_indices)\n",
    "testloader = torch.utils.data.DataLoader(subset_testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "dataloaders_dict = {'train':trainloader,'val':testloader}\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xS0TmGbXnMo",
    "outputId": "b9ea0162-8e9b-4e25-c56b-29448e9acc67",
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:42.038080Z",
     "start_time": "2024-04-26T13:15:40.319997Z"
    }
   },
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 224\n",
    "\n",
    "    return model_ft, input_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:42.042901Z",
     "start_time": "2024-04-26T13:15:42.038681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Learning Rate Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def create_optimizer(model, lr_main, lr_fc):\n",
    "    params_to_update = []\n",
    "    param_groups = [\n",
    "        {\"params\": [], \"lr\": lr_main},\n",
    "        {\"params\": [], \"lr\": lr_fc}\n",
    "    ]\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'fc' in name:\n",
    "                param_groups[1][\"params\"].append(param)\n",
    "            else:\n",
    "                param_groups[0][\"params\"].append(param)\n",
    "\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:42.049289Z",
     "start_time": "2024-04-26T13:15:42.041983Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def lr_experiments(lrs):\n",
    "    results = {\n",
    "        \"lr_main\": [],\n",
    "        \"lr_fc\": [],\n",
    "        \"scheduler_type\": [],\n",
    "        \"final_acc\": []\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "    for lr_main, lr_fc, scheduler_type in lrs:\n",
    "        print('Experiment {}, {}'.format(i + 1, lrs[i]))\n",
    "        \n",
    "        model_ft, input_size = initialize_model(num_classes, feature_extract = False)\n",
    "        model_ft = model_ft.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = create_optimizer(model_ft, lr_main, lr_fc)\n",
    "        if scheduler_type == \"step\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepL R(optimizer, step_size=100, gamma=0.1)\n",
    "        elif scheduler_type == \"exp\":\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "        elif scheduler_type == \"cosine\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "        elif scheduler_type == \"plateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, eps=1e-08)\n",
    "        elif scheduler_type == \"cycle\":\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=50, step_size_down=50)\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        trained_model, val_acc_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer, scheduler, num_epochs)\n",
    "        results[\"lr_main\"].append(lr_main)\n",
    "        results[\"lr_fc\"].append(lr_fc)\n",
    "        results[\"scheduler_type\"].append(scheduler_type)\n",
    "        results[\"final_acc\"].append(val_acc_hist[-1])\n",
    "        i += 1\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:15:42.055174Z",
     "start_time": "2024-04-26T13:15:42.045307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1, (0.001, 0.01, 'step')\n",
      "Epoch 1/8\n",
      "----------\n",
      "train Loss: 2.4198 Acc: 0.2600\n",
      "val Loss: 5.5579 Acc: 0.0000\n",
      "\n",
      "Epoch 2/8\n",
      "----------\n",
      "train Loss: 3.3162 Acc: 0.2200\n",
      "val Loss: 6.0407 Acc: 0.0000\n",
      "\n",
      "Epoch 3/8\n",
      "----------\n",
      "train Loss: 3.4929 Acc: 0.2200\n",
      "val Loss: 5.6835 Acc: 0.2000\n",
      "\n",
      "Epoch 4/8\n",
      "----------\n",
      "train Loss: 3.4661 Acc: 0.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/__init__.py\", line 23, in <module>\n",
      "    from .dropout import Dropout, Dropout1d, Dropout2d, Dropout3d, AlphaDropout, FeatureAlphaDropout\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/dropout.py\", line 246, in <module>\n",
      "    class FeatureAlphaDropout(_DropoutNd):\n",
      "  File \"/Users/tobiaspeihengli/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/dropout.py\", line 293, in FeatureAlphaDropout\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[125], line 14\u001B[0m\n\u001B[1;32m      1\u001B[0m learning_rates_schedulers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     (\u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      3\u001B[0m     (\u001B[38;5;241m0.0001\u001B[39m, \u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexp\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m     (\u001B[38;5;241m0.001\u001B[39m, \u001B[38;5;241m0.0001\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcycle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m ]\n\u001B[0;32m---> 14\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mlr_experiments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_rates_schedulers\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[124], line 31\u001B[0m, in \u001B[0;36mlr_experiments\u001B[0;34m(lrs)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m     scheduler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m trained_model, val_acc_hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr_main\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(lr_main)\n\u001B[1;32m     33\u001B[0m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr_fc\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(lr_fc)\n",
      "Cell \u001B[0;32mIn[118], line 24\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[1;32m     21\u001B[0m running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Iterate over data.\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m dataloaders[phase]:\n\u001B[1;32m     26\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     27\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:439\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:387\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:1040\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1033\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1040\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetbuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates_schedulers = [\n",
    "    (0.001, 0.01, \"step\"),\n",
    "    (0.0001, 0.01, \"exp\"),\n",
    "    (0.0001, 0.01, \"cosine\"),\n",
    "    (0.001, 0.01, \"plateau\"),\n",
    "    (0.001, 0.01, \"cycle\"),\n",
    "    (0.001, 0.0001, \"step\"),\n",
    "    (0.0001, 0.0001, \"exp\"),\n",
    "    (0.0001, 0.0001, \"cosine\"),\n",
    "    (0.001, 0.0001, \"plateau\"),\n",
    "    (0.001, 0.0001, \"cycle\")\n",
    "]\n",
    "\n",
    "results = lr_experiments(learning_rates_schedulers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T13:16:12.399478Z",
     "start_time": "2024-04-26T13:15:42.054258Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fine Tuning Layers Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model(num_classes, layers_to_tune):\n",
    "    # Load a pretrained model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze all layers in the network\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Layer groups in ResNet18\n",
    "    layer_names = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "    for layer_index in layers_to_tune:\n",
    "        for param in getattr(model, layer_names[layer_index - 1]).parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Replace the final fully connected layer (unfrozen)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)  # CIFAR10 has 10 classes\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_params_to_update(model):\n",
    "  params_to_update = [param for param in model.parameters() if param.requires_grad]\n",
    "\n",
    "  return params_to_update\n",
    "\n",
    "def run_layer_fine_tuning_experiments():\n",
    "    experiments = [\n",
    "        {\"layers_to_tune\": [4]},\n",
    "        {\"layers_to_tune\": [3, 4]},\n",
    "        {\"layers_to_tune\": [2, 3, 4]},\n",
    "        {\"layers_to_tune\": [1, 2, 3, 4]},\n",
    "    ]\n",
    "    i=1\n",
    "    for experiment in experiments:\n",
    "        model = get_model(num_classes, experiment[\"layers_to_tune\"])\n",
    "        params_to_update = get_params_to_update(model)\n",
    "        optimizer_ft = torch.optim.Adam(params_to_update, lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train and evaluate\n",
    "        print(\"Experiment {} with layer number {} tuned\".format(i, str(experiment[\"layers_to_tune\"])))\n",
    "        model_ft = train_model(model, dataloaders_dict, criterion, optimizer_ft, scheduler=None, num_epochs=num_epochs)\n",
    "        print(f'Experiment with layers {experiment[\"layers_to_tune\"]} completed.')\n",
    "        i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-26T13:16:12.395288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_layer_fine_tuning_experiments()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-26T13:16:12.397187Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Augmentation Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def data_augmentation_experiments(num_classes, augmentation_types, lr_main, lr_fc):\n",
    "    results = {\n",
    "        \"augmentation_type\": [],\n",
    "        \"final_acc\": []\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "    for augmentation_type in augmentation_types:\n",
    "        print('Experiment {}: {}'.format(i + 1, augmentation_type))\n",
    "        model_ft, input_size = initialize_model(num_classes, feature_extract = False)\n",
    "        model_ft = model_ft.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = create_optimizer(model_ft, lr_main, lr_fc)\n",
    "        # scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        # Update transforms based on augmentation type\n",
    "        transform = get_transform(augmentation_type)\n",
    "        full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                     download=True, transform=transform)\n",
    "        subset_size = int(0.001 * len(full_dataset))\n",
    "        subset_indices = torch.randperm(len(full_dataset))[:subset_size]\n",
    "        subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "        trainloader = torch.utils.data.DataLoader(subset_dataset, batch_size=batch_size,\n",
    "                                                  shuffle=True, num_workers=2)\n",
    "        dataloaders_dict = {'train': trainloader, 'val': testloader}\n",
    "        \n",
    "        trained_model, val_acc_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer, scheduler=None, num_epochs=num_epochs)\n",
    "        results[\"augmentation_type\"].append(augmentation_type)\n",
    "        results[\"final_acc\"].append(val_acc_hist[-1])\n",
    "        i += 1\n",
    "    return results\n",
    "\n",
    "def get_transform(augmentation_type):\n",
    "    if augmentation_type == \"flip\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    elif augmentation_type == \"rotation\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    elif augmentation_type == \"crops\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.RandomCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    elif augmentation_type == \"scaling\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((int(img_size * 1.1), int(img_size * 1.1))),\n",
    "            transforms.RandomCrop((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported augmentation type\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T15:09:44.362856Z",
     "start_time": "2024-04-26T15:09:44.359151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1: flip\n",
      "Files already downloaded and verified\n",
      "Epoch 1/8\n",
      "----------\n",
      "train Loss: 2.6370 Acc: 0.0800\n",
      "val Loss: 1.9628 Acc: 0.3000\n",
      "\n",
      "Epoch 2/8\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[129], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m lr_fc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[1;32m      4\u001B[0m augmentation_types \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflip\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrotation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrops\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscaling\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 5\u001B[0m data_augmentation_results \u001B[38;5;241m=\u001B[39m \u001B[43mdata_augmentation_experiments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugmentation_types\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_main\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_fc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(data_augmentation_results)\n",
      "Cell \u001B[0;32mIn[128], line 26\u001B[0m, in \u001B[0;36mdata_augmentation_experiments\u001B[0;34m(num_classes, augmentation_types, lr_main, lr_fc)\u001B[0m\n\u001B[1;32m     22\u001B[0m trainloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(subset_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m     23\u001B[0m                                           shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     24\u001B[0m dataloaders_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m: trainloader, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m: testloader}\n\u001B[0;32m---> 26\u001B[0m trained_model, val_acc_hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maugmentation_type\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(augmentation_type)\n\u001B[1;32m     28\u001B[0m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfinal_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(val_acc_hist[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "Cell \u001B[0;32mIn[118], line 42\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[1;32m     39\u001B[0m     _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 42\u001B[0m         \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     45\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define learning rates\n",
    "lr_main = 0.001\n",
    "lr_fc = 0.001\n",
    "augmentation_types = [\"flip\", \"rotation\", \"crops\", \"scaling\"]\n",
    "data_augmentation_results = data_augmentation_experiments(num_classes, augmentation_types, lr_main, lr_fc)\n",
    "print(data_augmentation_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T15:10:04.140137Z",
     "start_time": "2024-04-26T15:09:45.800442Z"
    }
   }
  }
 ]
}
